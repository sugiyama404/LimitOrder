{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sugiyama404/LimitOrder/blob/main/LimitOrder_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrOsXvniJGQw",
        "outputId": "8a2b54dd-579c-4106-a2f0-a7ed8826d573"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "import copy\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as kl\n",
        "from time import sleep\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
        "from multiprocessing import Manager\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "name = 'limitorder'\n",
        "mode = 'train'\n",
        "long_or_short = ['long', 'short']\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "out_dir = 'Colab Notebooks/dataset/LimitOrder/'\n",
        "csv_path = '/content/drive/My Drive/' + out_dir + f'predict_and_dataset_{mode}.csv'\n",
        "mdl_long_path = '/content/drive/My Drive/' + out_dir + f'model_{name}_{long_or_short[0]}.h5'\n",
        "mdl_short_path = '/content/drive/My Drive/' + out_dir + f'model_{name}_{long_or_short[1]}.h5'\n",
        "\n",
        "df = pd.read_csv(csv_path, index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eXmpwu0q2jXG"
      },
      "outputs": [],
      "source": [
        "class Environment:\n",
        "    def __init__(self, df, initial_money=100000):\n",
        "\n",
        "        self.df = df\n",
        "        self.df_total_steps  = len(self.df)-1\n",
        "        self.initial_money   = initial_money\n",
        "        self.cash_in_hand    = None\n",
        "        # hold, good_buy, good_sell, bad_buy, bad_sell, buy_cancel, sell_cancel\n",
        "        self.action_space    = np.array([0, 1, 2, 3, 4, 5, 6])\n",
        "        self.now_step  = None\n",
        "        self.alpha = 0.001\n",
        "        self.beta = 0.9\n",
        "        self.minimum_number_of_shares = 10000 # 最低取得株数\n",
        "\n",
        "        self.is_cancel = False\n",
        "        self.profit_and_loss = None\n",
        "        self.reserve_price = None\n",
        "        self.reserve_type = None\n",
        "        self.have_a_position = None\n",
        "        self.have_a_position_type = None\n",
        "        self.price_old = None\n",
        "        self.Money_old = None\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.now_step  = 0\n",
        "        self.now_price = (self.df['PRICE_ASK_0'][self.now_step] + self.df['PRICE_BID_0'][self.now_step]) / 2\n",
        "        self.cash_in_hand = self.initial_money / self.minimum_number_of_shares\n",
        "        self.hold_a_position = 0\n",
        "        self.end_step        = self.df_total_steps\n",
        "\n",
        "        self.is_cancel = False\n",
        "        self.profit_and_loss = []\n",
        "        self.reserve_price = 0\n",
        "        self.reserve_type = 'none'\n",
        "        self.have_a_position = 0\n",
        "        self.have_a_position_type = None\n",
        "\n",
        "        self.price_old = 0\n",
        "        self.Money_old = 0\n",
        "\n",
        "        return self._get_now_state()\n",
        "\n",
        "    def step(self, action):\n",
        "        '''\n",
        "        状態の内4まで出力する。\n",
        "        1. 現在ポジション株数/最大保有株数\n",
        "        2. 既存買注文の反対気配との乖離率 (逆数)\n",
        "        3. 既存売注文の反対気配との乖離率 (逆数)\n",
        "        4. Bid-Ask スプレッド\n",
        "        5. 短期(5 秒)リターン予測値\n",
        "        6. 長期(60 秒)リターン予測値\n",
        "\n",
        "        最大保有株数は 100 万円相当の株数(最小取引単位の\n",
        "        100 株に満たない場合は 100 株)で, \n",
        "        ショートポジションも取り得るものとしている.\n",
        "\n",
        "        取引エージェントの行動タイミングは各銘柄のティッ ク更新時に, \n",
        "        前回行動から 1 秒以上経過している場合に\n",
        "        発注判断を行うものとする (各発注判断を 1 ステップと する).\n",
        "        ある時点において市場に出すことのできる注 文は買い,\n",
        "        売りのそれぞれに対して1注文のみとし,最大ポジションを超える新規注文は行わない.\n",
        "        各時点で選ぶことのできない行動は選択肢から排除している.\n",
        "\n",
        "        報酬\n",
        "        r(t) = ∆PnL(t + 1) − α(∆Pos(t + 1))^2 − βFo(t)\n",
        "        ∆PnL(t + 1) : (累積) 損益額の変化幅(円)\n",
        "        ∆Pos(t + 1) : ポジション金額の変化幅 (円)\n",
        "        Fo(t) : 1 (時刻 t でキャンセルが発生した場合に追加を行う。)\n",
        "              : 0 (その他)\n",
        "        損益額のみを報酬とする場合, ポジションが必要以上\n",
        "        に増加する傾向にあり [3], また不必要な注文の送信, \n",
        "        取消を繰り返してしまう. そこで過度なポジショニングと\n",
        "        注文取消を抑制するために調整項を加えている.\n",
        "        '''\n",
        "        before_profit_and_loss = copy.deepcopy(self._get_revenue())\n",
        "        before_hold_a_position = copy.deepcopy(self.hold_a_position)\n",
        "        before_now_price = copy.deepcopy(self._get_now_price())\n",
        "\n",
        "        self.now_step += 1\n",
        "        self.now_price = self._get_now_price()\n",
        "\n",
        "        self._judge_the_execution() # 約定しているかを調べる。\n",
        "        self._order(action) # 注文を出す。\n",
        "\n",
        "\n",
        "        state = self._get_now_state()\n",
        "        reward = self._get_reward(before_profit_and_loss, before_hold_a_position, before_now_price)\n",
        "        done = (self.end_step == self.now_step)\n",
        "        info = {'now_step': self.now_step}\n",
        "\n",
        "        return state, reward, done, info\n",
        "\n",
        "    def _get_now_state(self):\n",
        "        state = np.empty(6)\n",
        "        state[0] = self.hold_a_position\n",
        "        state[1] = self.df['PRICE_ASK_0'][self.now_step] / self.df['PRICE_ASK_9'][self.now_step]\n",
        "        state[2] = self.df['PRICE_BID_0'][self.now_step] / self.df['PRICE_BID_9'][self.now_step]\n",
        "        state[3] = self.df['PRICE_ASK_0'][self.now_step] - self.df['PRICE_BID_0'][self.now_step]\n",
        "        state[4] = self.df['SHORT_PREDICT'][self.now_step]\n",
        "        state[5] = self.df['LONG_PREDICT'][self.now_step]\n",
        "        return state\n",
        "\n",
        "    def _get_reward(self, before_profit_and_loss, before_hold_a_position, before_now_price):\n",
        "        pnl = self._get_revenue() - before_profit_and_loss\n",
        "        # pnl = self._get_revenue()\n",
        "        pos = self.hold_a_position * self.now_price - before_hold_a_position * before_now_price\n",
        "        f0  = 1 if self.is_cancel else 0\n",
        "\n",
        "        reward = pnl - self.alpha * (pos ** 2) - self.beta * f0\n",
        "        return reward\n",
        "\n",
        "    def _order(self, action):\n",
        "        '''\n",
        "        取引エージェントの行動タイミングは各銘柄のティック更新時に, \n",
        "        前回行動から 1 秒以上経過している場合に\n",
        "        発注判断を行うものとする (各発注判断を 1 ステップとする).\n",
        "        ある時点において市場に出すことのできる注文は買い,\n",
        "        売りのそれぞれに対して1注文のみとし,最大ポジションを超える新規注文は行わない.\n",
        "        各時点で選ぶことのできない行動は選択肢から排除している.\n",
        "        '''\n",
        "        self.is_cancel = False\n",
        "\n",
        "        if self.action_space[0] == action: # 何もしない\n",
        "            pass\n",
        "        elif self.action_space[1] == action: # 最良気配への新規買注文送出/既存買注文訂正\n",
        "            price = self.df['PRICE_ASK_0'][self.now_step]\n",
        "            order_type = 'buy'\n",
        "        elif self.action_space[2] == action: # 最良気配への新規売注文送出/既存売注文訂正\n",
        "            price = self.df['PRICE_BID_0'][self.now_step]\n",
        "            order_type = 'sell'\n",
        "        elif self.action_space[3] == action: # 反対気配への新規買注文送出/既存買注文訂正\n",
        "            price = self.df['PRICE_ASK_9'][self.now_step]\n",
        "            order_type = 'buy'\n",
        "        elif self.action_space[4] == action: # 反対気配への新規売注文送出/既存売注文訂正\n",
        "            price = self.df['PRICE_BID_9'][self.now_step]\n",
        "            order_type = 'sell'\n",
        "        elif self.action_space[5] == action: # 既存買注文の取消\n",
        "            price = 0\n",
        "            order_type = 'buy'\n",
        "            self.is_cancel = True\n",
        "        else: # 既存売注文の取消\n",
        "            price = 0\n",
        "            order_type = 'sell'\n",
        "            self.is_cancel = True\n",
        "\n",
        "        if self.action_space[0] != action:\n",
        "            self._reservation_change(price, order_type)\n",
        "\n",
        "    def _reservation_change(self, price, order_type):\n",
        "        if not self.is_cancel:\n",
        "            self.reserve_price = price\n",
        "            self.reserve_type = order_type    \n",
        "        else:\n",
        "            self.reserve_price = 0\n",
        "            self.reserve_type = 'none'\n",
        "\n",
        "    def _judge_the_execution(self):\n",
        "        '''\n",
        "        約定しているかを調べる。\n",
        "\n",
        "        取引エージェントの行動タイミングは各銘柄のティック更新時に, \n",
        "        前回行動から 1 秒以上経過している場合に\n",
        "        発注判断を行うものとする (各発注判断を 1 ステップとする).\n",
        "        ある時点において市場に出すことのできる注文は買い,\n",
        "        売りのそれぞれに対して1注文のみとし,最大ポジションを超える新規注文は行わない.\n",
        "        各時点で選ぶことのできない行動は選択肢から排除している.\n",
        "\n",
        "        self.minimum_number_of_shares = 10000 # 最低取得株数\n",
        "        '''\n",
        "\n",
        "        if self.have_a_position_type == 'buy':\n",
        "            '''買いのポジションを持っている場合'''\n",
        "            if self.reserve_price <= self.df['PRICE_ASK_0'][self.now_step] and self.reserve_type == 'sell':\n",
        "                self.cash_in_hand += self._bear_position_calc()\n",
        "                self._reserve_and_position_reset(True)\n",
        "                self.price_old = 0\n",
        "                self.Money_old = 0\n",
        "        elif self.have_a_position_type == 'sell':\n",
        "            '''売りのポジションを持っている場合'''\n",
        "            if self.reserve_price >= self.df['PRICE_BID_0'][self.now_step] and self.reserve_type == 'buy':\n",
        "                self.cash_in_hand += self.reserve_price * self.hold_a_position\n",
        "                self._reserve_and_position_reset(True)\n",
        "        else:\n",
        "            '''ポジションを持っていない場合'''\n",
        "            if self.reserve_type == 'buy':\n",
        "                if self.reserve_price >= self.df['PRICE_BID_0'][self.now_step] and self.cash_in_hand >= 0:\n",
        "                    buy_flag = True\n",
        "                    while buy_flag:\n",
        "                        if self.cash_in_hand > self.reserve_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            buy_flag = False\n",
        "                    self.have_a_position_type = 'buy'\n",
        "                    self._reserve_and_position_reset()\n",
        "            if self.reserve_type == 'sell':\n",
        "                if self.reserve_price <= self.df['PRICE_ASK_0'][self.now_step] and self.cash_in_hand >= 0:\n",
        "                    sell_flag = True\n",
        "                    self.price_old = self.reserve_price\n",
        "                    self.Money_old = self.cash_in_hand\n",
        "                    while sell_flag:\n",
        "                        if self.cash_in_hand > self.reserve_price:\n",
        "                            self.hold_a_position += 1\n",
        "                            self.cash_in_hand -= self.now_price\n",
        "                        else:\n",
        "                            sell_flag = False\n",
        "                    self.have_a_position_type = 'sell'\n",
        "                    self._reserve_and_position_reset()\n",
        "\n",
        "    def _reserve_and_position_reset(self, position_reset = False):\n",
        "            self.reserve_price = 0\n",
        "            self.reserve_type == 'none'\n",
        "            if position_reset:\n",
        "                self.hold_a_position = 0\n",
        "                self.have_a_position_type = 'none'\n",
        "\n",
        "    def _get_revenue(self):\n",
        "        if self.have_a_position_type == 'sell':\n",
        "            return self._bear_position_calc() + self.cash_in_hand\n",
        "        else:\n",
        "            return self.hold_a_position * self.now_price + self.cash_in_hand\n",
        "\n",
        "    def _bear_position_calc(self):\n",
        "        return ((self.price_old - self.now_price) * self.hold_a_position\n",
        "                + self.Money_old)\n",
        "        \n",
        "    def _get_now_price(self):\n",
        "        return (self.df['PRICE_ASK_0'][self.now_step] + self.df['PRICE_BID_0'][self.now_step]) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-iER-LTcS8rq"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(tf.keras.Model):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(NeuralNetwork, self).__init__(*args, **kwargs)\n",
        "\n",
        "        self.action_space = 7\n",
        "\n",
        "        self.input_layer = kl.Dense(20, activation=\"tanh\", kernel_initializer=\"he_normal\")\n",
        "        self.dense1 = kl.Dense(10, activation=\"tanh\", kernel_initializer=\"he_normal\")\n",
        "        self.value = kl.Dense(1, kernel_initializer=\"he_normal\")\n",
        "        self.dense2 = kl.Dense(10, activation=\"tanh\", kernel_initializer=\"he_normal\")\n",
        "        self.advantages = kl.Dense(self.action_space, kernel_initializer=\"he_normal\")\n",
        "\n",
        "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, training=None):\n",
        "        x = self.input_layer(x)\n",
        "        x1 = self.dense1(x)\n",
        "        value = self.value(x1)\n",
        "        x2 = self.dense2(x)\n",
        "        advantages = self.advantages(x2)\n",
        "        q = value + advantages - tf.reduce_mean(advantages, axis=1, keepdims=True)\n",
        "        return q\n",
        "\n",
        "    def save_model(self, name):\n",
        "        self.save_weights(name)\n",
        "\n",
        "    def load_model(self, name):\n",
        "        self.load_weights(name)\n",
        "\n",
        "    def update_model(self, value):\n",
        "        self.set_weights(value)\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.get_weights()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rMObQT3JFj7m"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, max_size, batch_size):\n",
        "        self.cntr = 0\n",
        "        self.size = 0\n",
        "        self.max_size = max_size\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.states_memory = np.zeros([self.max_size, 6], dtype=np.float32)\n",
        "        self.acts_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "        self.rewards_memory = np.zeros(self.max_size, dtype=np.float32)\n",
        "        self.next_states_memory = np.zeros([self.max_size, 6], dtype=np.float32)\n",
        "        self.dones_memory = np.zeros(self.max_size, dtype=np.uint8)\n",
        "        self.tderrors_memory = np.zeros(self.max_size, dtype=np.float32)\n",
        "\n",
        "    def add(self, exp):\n",
        "        self.states_memory[self.cntr] = exp.state\n",
        "        self.acts_memory[self.cntr] = exp.act\n",
        "        self.rewards_memory[self.cntr] = exp.reward\n",
        "        self.next_states_memory[self.cntr] = exp.next_state\n",
        "        self.dones_memory[self.cntr] = exp.done\n",
        "        self.tderrors_memory[self.cntr] = exp.tderrors_memory\n",
        "        self.cntr = (self.cntr+1) % self.max_size\n",
        "        self.size = min(self.size+1, self.max_size)\n",
        "\n",
        "    def sampling(self):\n",
        "        td_errors = [self.tderrors_memory[index] for index in range(len(self.tderrors_memory))]\n",
        "        p = self._make_prob(td_errors)\n",
        "        batch_indexes = np.random.choice(len(self.tderrors_memory), size=self.batch_size, p=p).tolist()\n",
        "\n",
        "        state      = np.array([self.states_memory[index] for index in batch_indexes])\n",
        "        action     = np.array([self.acts_memory[index] for index in batch_indexes])\n",
        "        reward     = np.array([self.rewards_memory[index] for index in batch_indexes])\n",
        "        next_state = np.array([self.next_states_memory[index] for index in batch_indexes])\n",
        "        done       = np.array([self.dones_memory[index] for index in batch_indexes])\n",
        "        \n",
        "        return {'state': state, 'next_state': next_state, 'reward': reward, 'action': action, 'done': done}\n",
        "\n",
        "    def _make_prob(self, td_errors, alpha=0.5, eps=np.float32(0.001)):\n",
        "        abs_td_errors = np.power(np.abs(td_errors) + eps, alpha)\n",
        "        return abs_td_errors / np.sum(abs_td_errors)\n",
        "\n",
        "    def set_transition(self, batch):\n",
        "        states, actions = batch['state'], batch['action']\n",
        "        rewards, next_states = batch['reward'], batch['next_state']\n",
        "        dones, tderrors = batch['done'], batch['tderrors']\n",
        "       \n",
        "        for state, action, reward, next_state, done, tderror in zip(states, actions, \n",
        "                                                                    rewards,next_states, dones, tderrors):\n",
        "            self.states_memory[self.cntr] = state\n",
        "            self.acts_memory[self.cntr] = action\n",
        "            self.rewards_memory[self.cntr] = reward\n",
        "            self.next_states_memory[self.cntr] = next_state\n",
        "            self.dones_memory[self.cntr] = done\n",
        "            self.tderrors_memory[self.cntr] = tderror\n",
        "            self.cntr = (self.cntr+1) % self.max_size\n",
        "            self.size = min(self.size+1, self.max_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KQeGKJdS7lDP"
      },
      "outputs": [],
      "source": [
        "def train(episodes, q, qlist):\n",
        "    n_actions = 7\n",
        "    gamma = 9.9e-1\n",
        "    max_size = 50_000\n",
        "    batch_size = 2048\n",
        "    episodes = episodes\n",
        "    target_update_interval = 100\n",
        "    train_interval_time = 10\n",
        "\n",
        "    buffer = ReplayBuffer(max_size, batch_size)\n",
        "\n",
        "    model = NeuralNetwork()\n",
        "    model_target = NeuralNetwork()\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "    while True:\n",
        "        if not q.empty():\n",
        "            batch = q.get()\n",
        "            buffer.set_transition(batch)\n",
        "        sleep(train_interval_time)\n",
        "        if buffer.size >= buffer.batch_size:\n",
        "            break           \n",
        "\n",
        "    for episode in range(episodes):\n",
        "        batch = buffer.sampling()\n",
        "\n",
        "        q_next = model(batch['next_state'])\n",
        "        q_target_next = model_target(batch['next_state'])\n",
        "\n",
        "        next_actions = tf.cast(tf.argmax(q_next, axis=1), tf.int32)\n",
        "        next_actions_onehot = tf.one_hot(next_actions, n_actions)\n",
        "\n",
        "        next_maxQ = tf.reduce_sum(\n",
        "            q_target_next * next_actions_onehot, axis=1, keepdims=True)\n",
        "        TQ = batch['reward'] + gamma * (1 - batch['done']) * next_maxQ\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            qvalues = model(batch['state'])\n",
        "            actions_onehot = tf.one_hot(batch['action'], n_actions)\n",
        "            Q = tf.reduce_sum(qvalues * actions_onehot, axis=1, keepdims=True)\n",
        "            td_errors = tf.square(TQ - Q)\n",
        "            loss = tf.reduce_mean(td_errors)\n",
        "\n",
        "        grads = tape.gradient(loss, model.trainable_variables)\n",
        "        grads, _ = tf.clip_by_global_norm(grads, 40.0)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        # workerにデータ送信\n",
        "        for ql in qlist:\n",
        "            ql.put(model.get_model())\n",
        "\n",
        "        if episode % target_update_interval == 0 and episode != 0:\n",
        "            model_target.update_model(model.get_model())\n",
        "            print(f'Learner training times: {str(episode)} / {str(episodes)}')\n",
        "        \n",
        "        for _ in range(4):\n",
        "            if not q.empty():\n",
        "                batch = q.get()\n",
        "                buffer.set_transition(batch)\n",
        "\n",
        "    # workerにデータ送信\n",
        "    for ql in qlist:\n",
        "        ql.put('final')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ExszBI8qLfpj"
      },
      "outputs": [],
      "source": [
        "class Experiences:\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        データ転送用クラス\n",
        "        agentのExperiencesを蓄える用\n",
        "        '''\n",
        "        self.state = np.zeros([0, 6], dtype=np.float32)\n",
        "        self.action = np.zeros(0, dtype=np.uint8)\n",
        "        self.reward = np.zeros(0, dtype=np.float32)\n",
        "        self.next_state = np.zeros([0, 6], dtype=np.float32)\n",
        "        self.done = np.zeros(0, dtype=np.uint8)\n",
        "        self.tderrors = np.zeros(0, dtype=np.float32)\n",
        "\n",
        "    def exp_reset(self):\n",
        "        self.state = np.zeros([0, 6], dtype=np.float32)\n",
        "        self.action = np.zeros(0, dtype=np.uint8)\n",
        "        self.reward = np.zeros(0, dtype=np.float32)\n",
        "        self.next_state = np.zeros([0, 6], dtype=np.float32)\n",
        "        self.done = np.zeros(0, dtype=np.uint8)\n",
        "        self.tderrors = np.zeros(0, dtype=np.float32)\n",
        "\n",
        "    def exp_add(self, state, action, reward, next_state, done):\n",
        "        state  = np.reshape(state, (1, 6))\n",
        "        self.state = np.append(self.state, state, axis=0)\n",
        "        self.action = np.append(self.action, np.array(action))\n",
        "        self.reward = np.append(self.reward, np.array(reward))\n",
        "        next_state  = np.reshape(next_state, (1, 6))\n",
        "        self.next_state = np.append(self.next_state, next_state, axis=0)\n",
        "        self.done =  np.append(self.done, np.array(done))\n",
        "\n",
        "    def transition(self):\n",
        "        key = ['state', 'action', 'reward', 'next_state', 'done', 'tderrors']\n",
        "        value = [self.state, self.action, self.reward,\n",
        "                 self.next_state, self.done, self.tderrors]\n",
        "        return dict(zip(key,value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vQpm-nUK3qrE"
      },
      "outputs": [],
      "source": [
        "class Agent(Experiences):\n",
        "    def __init__(self, epsilon):\n",
        "        self.model        = NeuralNetwork()\n",
        "        self.model_target = NeuralNetwork()\n",
        "\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = epsilon\n",
        "        self.n_actions = 7\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "    def act(self, state):\n",
        "        ran_num = np.random.rand()\n",
        "        if ran_num <= self.epsilon:\n",
        "            return np.random.choice(7)\n",
        "        else:\n",
        "            return self._predict(state)\n",
        "\n",
        "    def _predict(self, state):\n",
        "        act = self.model(state)\n",
        "        return np.argmax(act)\n",
        "\n",
        "    def make_tderror(self):\n",
        "        td_q = self.model_target(self.next_state)\n",
        "        q = self.model_target(self.state)\n",
        "        q_value = tf.reduce_sum(q * tf.one_hot(self.action, self.n_actions),\n",
        "                                axis = 1)\n",
        "        self.tderrors = self.reward + (1 - self.done) * self.gamma * np.max(td_q, axis=1) - q_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "V86ZrvF2uLrH"
      },
      "outputs": [],
      "source": [
        "def main(epsilon, agent_num, q, rq):\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "    env = Environment(df)\n",
        "    target_update_interval = 20\n",
        "    agent = Agent(epsilon)\n",
        "    terminal = False\n",
        "    cnt = 0\n",
        "    train_interval_time = env.df_total_steps // 5\n",
        "\n",
        "    while not terminal:\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        state = env.reset()\n",
        "\n",
        "        while not done:\n",
        "            action = agent.act(np.array([state]))\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            reward = reward_clipping(reward)\n",
        "            agent.exp_add(state, action, reward, next_state, done)\n",
        "            total_reward += reward\n",
        "\n",
        "            if len(agent.action) == train_interval_time:\n",
        "                agent.make_tderror()\n",
        "                trans = agent.transition()\n",
        "                q.put(trans)\n",
        "                agent.exp_reset()\n",
        "\n",
        "            if not rq.empty():\n",
        "                w = rq.get()\n",
        "                if w == 'final':\n",
        "                    terminal = True\n",
        "                    break\n",
        "                else:\n",
        "                    agent.model.update_model(w)\n",
        "                    if cnt % target_update_interval == 0:\n",
        "                        agent.model_target.update_model(agent.model.get_model())\n",
        "                    cnt += 1\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        if agent_num == 0:\n",
        "            print(f'Reward:{str(reward)} Total Reward:{str(total_reward)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reward_clipping(val):\n",
        "        result = 1 if val > 0 else 0 if val == 0 else -1\n",
        "        return result"
      ],
      "metadata": {
        "id": "g9HSHF6BBK6m"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25H3w1FUA4Aa",
        "outputId": "68c76053-f302-4b08-dfa8-d9b11abd1ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start worker_0\n",
            "start worker_1\n",
            "start worker_2\n",
            "start train\n",
            "Reward:0 Total Reward:-152\n",
            "Reward:0 Total Reward:-147\n",
            "Reward:0 Total Reward:-147\n",
            "Reward:-1 Total Reward:-22765\n",
            "Reward:-1 Total Reward:-153919\n",
            "Reward:0 Total Reward:-131124\n",
            "Reward:0 Total Reward:-145\n",
            "Reward:0 Total Reward:-145\n",
            "Reward:0 Total Reward:-145\n"
          ]
        }
      ],
      "source": [
        "worker_num = 3\n",
        "episodes = 10**4\n",
        "\n",
        "q = Manager().Queue(4) # 最初に入れたデータを最初に取り出す。\n",
        "qlist = [Manager().Queue(1) for _ in range(worker_num)]\n",
        "epsilon_list = [0, 0.8, 0.5]\n",
        "\n",
        "with ProcessPoolExecutor(max_workers=4) as executor:\n",
        "    data = []\n",
        "    for rq, e, i in zip(qlist, epsilon_list, range(worker_num)):\n",
        "        print(f'start worker_{i}')\n",
        "        data.append(executor.submit(main, e, i, q, rq))\n",
        "    print('start train')\n",
        "    data.append(executor.submit(train, episodes, q, qlist))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}